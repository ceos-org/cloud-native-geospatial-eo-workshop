{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Land Productivity with Harmonized Landsat and Sentinel-2: A Cloud Native Approach\n",
    "\n",
    "The cloud native geospatial paradigm has the potential to make Earth observation analysis accessible to more people, more easily.\n",
    "In the simplest terms, instead of downloading data before performing an analysis, it’s now possible to stream data directly from the cloud.\n",
    "\n",
    "This notebook walks through the steps required to query and load NASA's Harmonized Landsat and Sentinel-2 (HLS) data using a Spatio-Temportal Asset Catalog (STAC), and how to use that data to investigate land productivity.\n",
    "Importantly, the benefit of using a cloud native approach is that there is no requirement to download and process individual scenes. \n",
    "Instead, a query is used to identify the relevant scenes, and then software is used to only load a relevant portion of those scenes into the computer's memory. \n",
    "\n",
    "## Use case: measuring land productivity for SDG Indicator 15.3.1\n",
    "\n",
    "Sustainable Development Goal 15 is to \"Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\".\n",
    "Under this goal is Target 15.3, which stipulates \"By 2030, combat desertification, restore degraded land and soil, including land affected by desertification, drought and floods, and strive to achieve a land degradation-neutral world\".\n",
    "The relevant indicator is 15.3.1: \"Proportion of land that is degraded over total land area\", which has three sub-indicators: trends in land cover, trends in land productivity or functioning, and trends in carbon stocks above and below ground. \n",
    "\n",
    "![Flowchart demonstrating that three indicators make up SDG indicator 15.3.1](images/steps_for_indicator_1531.png \"SDG Indicator 15.3.1 Methodology\")\n",
    "\n",
    "*Figure 1. Steps to derive the indicator from the sub-indicators where ND is not degraded and D is degraded. Source: [Good practice guidence. SDG Indicator 15.3.1](https://www.unccd.int/resources/manuals-and-guides/good-practice-guidance-sdg-indicator-1531-proportion-land-degraded).*\n",
    "\n",
    "Earth observation data has a strong role to play in all three sub-indicators, and regular remote sensing of the environment from satellites can assist with looking at trends in both land cover and land productivity. \n",
    "As a demonstration, this notebook focusses on the land productivity subindicator, rather than the full 15.3.1 indicator.\n",
    "For more information, we recommend the following resources:\n",
    "\n",
    "* [Good practice guidance. SDG indicator 15.3.1, Proportion of land that is degraded over total land area. Version 2.0.](https://www.unccd.int/resources/manuals-and-guides/good-practice-guidance-sdg-indicator-1531-proportion-land-degraded)\n",
    "* [Satellite Data Requirements for SDGIndicator 15.3.1](https://ceos.org/sdg/files/supportsheets/SDG_15.3.1_EO_Satellite_Data_Requirements_31Aug2022.pdf)\n",
    "* [TRENDS.EARTH: tracking land change](https://maps.trends.earth/map?tab=layers&zoom=7&center=lat%3D-8.477805461808186%26lng%3D-67.87353515625001&layers=%5B%5D&basemap=satellite)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application area\n",
    "\n",
    "This notebook will calculate the trend in land productivity for a forested region in Capão Bonito, a municipality in the state of São Paulo in Brazil. \n",
    "Today, there are large areas of forest in the sourthern region, but this wasn't always the case. \n",
    "The video below shows the region from 1984 through to today, with large areas of grasslands converting to forests around 2008-2012.\n",
    "\n",
    "<iframe width=\"1080\" height=\"600\" src=\"https://earthengine.google.com/iframes/timelapse_player_embed.html#v=-24.11033,-48.2946,11.685,latLng&t=3.45&ps=50&bt=19840101&et=20221231\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\n",
    "This notebook uses NASA's Harmonized Landsat Sentinel-2 product, which provides imagery from 2013 onwards. \n",
    "As such, the purpose of this notebook is to examine whether the trend in land productivity has been stable since the reforesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook overview\n",
    "\n",
    "This notebook demonstrates how to use Earth observation data from satellites to measure the trend in land productivity for a region. The notebook is broken into three parts, as follows.\n",
    "\n",
    "### Part 1: Querying and loading\n",
    "\n",
    "1. Setting up the notebook\n",
    "2. Querying NASA's Earthdata STAC catalog to identify relevent scenes from the last year\n",
    "3. Loading data from the identified scenes\n",
    "\n",
    "### Part 2: Preparing data and identifying the peak productivity period\n",
    "\n",
    "1. Removing clouds from the loaded data\n",
    "2. Measuring land productivity per pixel using a vegetation index as a proxy\n",
    "3. Calculating the maximum vegetation index value for each month\n",
    "4. Identifying the period of peak productivity by looking at the average maximum value for the area in each month\n",
    "\n",
    "### Part 3: Land productivity trend from 2013 to 2024\n",
    "\n",
    "1. Querying NASA's Earthdata STAC to identify relevent scenes from the peak productivity period of each year since 2013\n",
    "2. Loading the data from the identified scenes\n",
    "3. Applying cloud masking and calculating the vegetation index\n",
    "4. Measuring the maximum vegetation index value for the peak period of each year\n",
    "5. Observing the change in maximum vegetation index value over the last ten years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Querying and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Setting up the notebook\n",
    "\n",
    "The first step is to set up the requried Python libraries and local imports.\n",
    "\n",
    "* `json` is used to load your Earthdata token\n",
    "* `odc.stac` and `pystac_client` are used to access the NASA Earthdata STAC\n",
    "* `geopandas` and `numpy` are used to manipulate data\n",
    "* `utils` is a local module that contains a configuration dictionary for Harmonized Landsat and Sentinel-2 (HLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from odc.stac import configure_rio, load\n",
    "from pystac_client import Client as PystacClient\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "from utils import hls_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to start a Dask client.\n",
    "\n",
    "Dask supports local parallel processing and can help speed up computation times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client as DaskClient\n",
    "\n",
    "dask_client = DaskClient()\n",
    "dask_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Querying NASA's CMR STAC catalog to identify relevent scenes from the last year\n",
    "\n",
    "NASA's Earthdata STAC catalog contains structured metadata that can be used by software to find relevant Landsat and Sentinel-2 scenes that match a user's requested area of interest and time frame.\n",
    "\n",
    "The first step is to specify the catalog and then connect to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The catalog URL for the Earthdata STAC containing HLS\n",
    "catalog = \"https://cmr.earthdata.nasa.gov/cloudstac/LPCLOUD/\"\n",
    "\n",
    "# pystac_client is used to connect to the catalog\n",
    "stac_client = PystacClient.open(catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to specify which collections to connect to.\n",
    "In this case, HLS is made up of two collections:\n",
    "\n",
    "* Harmonized Landsat at 30m resolution: `HLSL30.v2.0`\n",
    "* Harmonized Sentinel-2 at 30m resolution: `HLSS30.v2.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify both Landsat and Sentinel-2 HLS collections for the query\n",
    "collections = [\"HLSL30.v2.0\", \"HLSS30.v2.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to specify an area of interest, a forested region in Capão Bonito, a municipality in the state of São Paulo in Brazil. \n",
    "\n",
    "This notebook uses geopandas to read the area of interest as a polygon from a GeoJSON file. \n",
    "\n",
    "* `aoi` is the full GeoDataFrame\n",
    "* `aoi_geom` is the polygon of the first item in the GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the area of interest as a GeoDataFrame using geopandas\n",
    "aoi = gpd.read_file(\"aoi.geojson\")\n",
    "\n",
    "# Separate out the polygon for the first item in the GeoDataFrame\n",
    "aoi_geom = aoi.iloc[0].geometry\n",
    "\n",
    "# View the area of interest on an interactive map\n",
    "aoi.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to specify a date range. \n",
    "\n",
    "For part 1 and part 2, the notebook will use one year worth of data.\n",
    "\n",
    "* `start_date` and `end_date` are provided as YYYY-MM-DD strings\n",
    "* `date_range` is a string used in the query, which has the start and end dates separated by `/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the start date and end date\n",
    "start_date = \"2022-06-01\"\n",
    "end_date = \"2023-06-01\"\n",
    "\n",
    "# Format the date range for the query\n",
    "date_range = f\"{start_date}/{end_date}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to use the `client` connection to connect to the Earthdata STAC and search for all Landsat and Sentinel-2 scenes that intersect with the area of interest polygon within the specified date range.\n",
    "In the terminology of STAC, each scene is an `item`.\n",
    "\n",
    "Once complete, the code will print out the number of items that matched the query parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for items in the collection\n",
    "items = list(\n",
    "    stac_client.search(\n",
    "        collections=collections, intersects=aoi_geom, datetime=date_range\n",
    "    ).items()\n",
    ")\n",
    "\n",
    "print(f\"Found {len(items)} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Loading data from the identified scenes\n",
    "\n",
    "After finding the relevant STAC items, it is now possible to load the relevant data directly from those items.\n",
    "\n",
    "The first step is to set up authentication to allow access to the data contained within NASA's Earthdata STAC. \n",
    "Until this point, the notebook has only queried the metadata stored in the STAC.\n",
    "\n",
    "The [earthaccess](https://github.com/nsidc/earthaccess) library wraps common methods for authorizing, searching and accessing data on NASA's Earthdata cloud. Before using earthaccess, you need to register for an account with NASA's Earthdata system. The next line will prompt you for your username and password (at the top of the browser if using Github Codespaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "\n",
    "earthaccess.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract just the token from our logged in `earthaccess` instance so we can use it with rasterio via odc.stac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will be wrapped by `get_edl_token` in the next earthaccess release\n",
    "token = earthaccess.__auth__.token[\"access_token\"]\n",
    "\n",
    "# Configure rasterio to use cloud defaults, and GDAL to use the authorization token\n",
    "header_string = f\"Authorization: Bearer {token}\"\n",
    "configure_rio(cloud_defaults=True, GDAL_HTTP_HEADERS=header_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to use `odc.stac` to lazily load the queried data. \n",
    "Here, lazy loading is provided by Dask and means that it is possible to see information about the dimensionality and type of data, without pulling all the numerical satellite data. \n",
    "\n",
    "The `load` command from `odc.stac` has a number of arguments:\n",
    "* `items` are the scenes identified by querying NASA's Earthdata STAC\n",
    "* `resolution` specifies the resolution (in metres) for the loaded data\n",
    "* `crs` specifies the coordinate reference system for the loaded data\n",
    "* `chunks={}` specifies that lazy loading should be performed\n",
    "* `groupby=\"solarday\"` specifies that scenes that occur on the same day should be grouped\n",
    "* `stac_cfg` specifies any additional configuration requried to load the data. See the `utils.py` file for details\n",
    "* `bands` specifies the spectral and quality bands to load\n",
    "* `geopolygon` specifies the area of interest to load\n",
    "\n",
    "When the lazy load is complete, a nicely formatted summary of the lazy-loaded data will appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load(\n",
    "    items,\n",
    "    resolution=30,\n",
    "    crs=\"EPSG:5530\",\n",
    "    chunks={},\n",
    "    groupby=\"solar_day\",\n",
    "    stac_cfg=hls_config,\n",
    "    bands=[\"red\", \"green\", \"blue\", \"nir\", \"fmask\"],\n",
    "    geopolygon=aoi_geom,\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to have a quick look at the data using a custom function for making a true color image. \n",
    "Any visualisation will require data to be loaded into memory, so the following step will take a moment to read the necessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple plotting function to reuse\n",
    "def plot_rgb(data):\n",
    "    # Select the red green and blue bands\n",
    "    rgb = data[[\"red\", \"green\", \"blue\"]]\n",
    "\n",
    "    # Select a subset of images that show clear and cloudy images\n",
    "    rgb_subset = rgb.isel(time=slice(4, 7)).to_array()\n",
    "\n",
    "    # Display the image\n",
    "    rgb_subset.plot.imshow(col=\"time\", col_wrap=3, vmin=0, vmax=2000)\n",
    "\n",
    "\n",
    "plot_rgb(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparing data and identifying the peak productivity period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Removing clouds from the loaded data\n",
    "\n",
    "When displaying three timesteps of the loaded data in the previous section, it was evident that some images contain clouds.\n",
    "Rather than throw cloud-affected images away, it is possible to use quality information (provided in the `fmask` band) to remove the offending pixels. \n",
    "\n",
    "For this, a pixel quality mask must be constructed that identifies the locations of cloud and cloud shadow. The `fmask` band contains integer values that encode the quality of the pixel using bit-flags, as shown in Figure 2.\n",
    "\n",
    "![Diagram showing which bits map to each pixel quality flag](images/cloudmask.jpg \"Bit-masking using fmask\")\n",
    "\n",
    "*Figure 2: Bits are read from right to left. A value of 1 indicates True, a value of 0 indicates False. In this case, the binary digit will match against any pixels that are classified as cloud, adjacent cloud, or cloud shadow. For more information see the [HLS User Guide](https://lpdaac.usgs.gov/documents/1698/HLS_User_Guide_V2.pdf)*\n",
    "\n",
    "The following cell uses a number of variables:\n",
    "\n",
    "* `pq_bin` is the binary digit with values of `1` for cloud, adjacent cloud, and cloud shadow\n",
    "* `pq_mask` is a True/False array with a value of True if the `fmask` value has flags that match any of the `1` flags in `pq_bin`\n",
    "* `nodata_mask` is a True/False array with a value of True if the `fmask` value matches the no data value\n",
    "* `mask` is a True/False array with a value of True if the pixel belongs to either the `pq_mask` or the `nodata_mask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to reuse\n",
    "def apply_quality_mask(data):\n",
    "    # Specify pixel quality mask by setting the binary flags\n",
    "    # bit 1 (cloud), 2 (adjacent cloud), and 3 (cloud shadow) to True\n",
    "    pq_bin = 0b00001110\n",
    "    pq_mask = (data.fmask & pq_bin) != 0\n",
    "\n",
    "    # Specify the no data mask as any pixel that is equal to the nodata value in the metadata\n",
    "    nodata_mask = data.fmask == data.fmask.odc.nodata\n",
    "\n",
    "    # Combine and apply both masks\n",
    "    # If the mask is True, replace the pixel value with nan\n",
    "    mask = pq_mask | nodata_mask\n",
    "    masked = data.where(~mask, other=np.nan)\n",
    "    masked.drop_vars([\"fmask\"])\n",
    "\n",
    "    return masked\n",
    "\n",
    "\n",
    "masked = apply_quality_mask(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to plot the masked data to see the effect of the masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the masked data\n",
    "plot_rgb(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Measuring land productivity per pixel using a vegetation index as a proxy\n",
    "\n",
    "For SDG Indicator 15.3.1, a vegetation index is used as a proxy for land productivity. \n",
    "\n",
    "The first step is to apply scaling to the relevant bands -- HLS values are supplied as integers, but it is best practice to scale them to have values between 0 and 1 when doing calculations. HLS has a scale factor of `0.0001`. The cell below defines a function for this, and also sets any negative values to `nan`.\n",
    "\n",
    "After that, a vegetation index can be calculated. \n",
    "There are multiple vegetation indices that can be used for land productivity. \n",
    "This example uses the two-band enhanced vegetation index (EVI2), which is preferred for regions of high biomass.\n",
    "\n",
    "$$\\text{EVI2} = 2.4 \\times \\frac{\\text{NIR} - \\text{Red}}{\\text{NIR} + \\text{Red} + 1}$$\n",
    "\n",
    "The index has values between -1 and 1, with values closer to 1 indicating dense, productive vegetation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_offset(band):\n",
    "    # Apply a scaling factor of 0.0001 and only keep values greater than 0\n",
    "    band = band * 0.0001\n",
    "    band = band.where(band > 0, other=np.nan)\n",
    "\n",
    "    return band\n",
    "\n",
    "\n",
    "def calculate_evi2(data):\n",
    "    nir = scale_offset(data.nir)\n",
    "    red = scale_offset(data.red)\n",
    "\n",
    "    evi2 = 2.4 * ((nir - red) / (nir + red + 1))\n",
    "\n",
    "    return evi2\n",
    "\n",
    "\n",
    "masked[\"evi2\"] = calculate_evi2(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to plot the calculated index.\n",
    "The next cell defines a function that displays the same time steps used for the true color image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evi2(data):\n",
    "    data.evi2.isel(time=slice(4, 7)).plot.imshow(\n",
    "        col=\"time\", col_wrap=3, cmap=\"RdYlGn\")\n",
    "\n",
    "\n",
    "plot_evi2(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Calculating the maximum vegetation index value for each month\n",
    "\n",
    "Rather than removing scenes with cloud cover, it is valuable to combine multiple scenes into a single representative scene.\n",
    "For calculating the trend in land productivity, the metric used is the maximum vegetation index value.\n",
    "\n",
    "The following cell applies a resampling operation to group scenes by month, and then applies the maximum. \n",
    "\n",
    "At this time, it is also useful to load the relevant data into memory. \n",
    "This is done by using the `.compute()` method on the lazy loaded data. \n",
    "To see the processing, click the Dask Dashboard URL that was generated at the start of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a monthly maximum now we have masked data\n",
    "max_values = masked.resample(time=\"1MS\").max()\n",
    "\n",
    "# Get Dask to run processing\n",
    "max_evi2 = max_values.evi2.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to display the monthly maximum index value over time and look for changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_evi2.plot.imshow(col=\"time\", col_wrap=4, cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4  Identifying the period of peak productivity by looking at the average maximum value for the area in each month\n",
    "\n",
    "Rather than looking at the spatial information, it is possible to summarise the data further into an average value for the whole area in each month. \n",
    "This helps identify the general patterns in the vegetation index, such as the period of peak productivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_evi2.mean([\"x\", \"y\"]).plot(ylim=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time series shows that the index is lowest during September, and the index is at a peak between December and February. \n",
    "From this, we can select the peak productivity period as December to February.\n",
    "Using observations from a three month period will also likely help mitigate the cloud cover that appears frequently in January."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Land productivity trend from 2013 to 2024\n",
    "\n",
    "The important measure of land productivity in terms of land degredation is related to how it is trending through time.\n",
    "The purpose of identifying the peak productivity period is to then load data from many years during the same period and compare these over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Querying NASA's Earthdata STAC to identify relevent scenes from the peak productivity period of each year since 2013\n",
    "\n",
    "The first step is to identify all scenes that were captured between the start of December and the end of February for a given year. \n",
    "This is done by constructing a loop in the cell below, starting at 2013 (earliest available HLS data) and going through to 2024.\n",
    "\n",
    "The loop identifies all scenes in each year and appends them to the list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_range = range(2013, 2024)\n",
    "\n",
    "peak_vegetation_items = []\n",
    "\n",
    "for year in year_range:\n",
    "    start_date = f\"{year}-12-01\"\n",
    "    end_date = f\"{year+1}-02-28\"\n",
    "    date_range = f\"{start_date}/{end_date}\"\n",
    "\n",
    "    # Search for items in the collection\n",
    "    items = list(\n",
    "        stac_client.search(\n",
    "            collections=collections, intersects=aoi_geom, datetime=date_range\n",
    "        ).items()\n",
    "    )\n",
    "\n",
    "    peak_vegetation_items.extend(items)\n",
    "\n",
    "print(f\"Found {len(peak_vegetation_items)} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Loading the data from the identified scenes\n",
    "\n",
    "As in Part 1, the next step is to use `odc.stac` to lazily load the queried data.\n",
    "\n",
    "Apart from the use of `peak_vegetation_items` as the list of items to load, all other arguments keep the same values as the loading step used in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_data = load(\n",
    "    peak_vegetation_items,\n",
    "    resolution=30,\n",
    "    crs=\"EPSG:5530\",\n",
    "    chunks={},\n",
    "    groupby=\"solar_day\",\n",
    "    stac_cfg=hls_config,\n",
    "    bands=[\"red\", \"green\", \"blue\", \"nir\", \"fmask\"],\n",
    "    geopolygon=aoi,\n",
    ")\n",
    "\n",
    "trend_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Applying masking and calculating the vegetation index\n",
    "\n",
    "As in Part 2, it is important to apply the quality mask to the data before calculating the index.\n",
    "This section reuses the functions for masking and calculating the index that were used in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_data_masked = apply_quality_mask(trend_data)\n",
    "\n",
    "trend_data_masked[\"evi2\"] = calculate_evi2(trend_data_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Measuring the maximum vegetation index value for the peak period of each year\n",
    "\n",
    "The next step is to calculate the maximum value for each three month period, starting in December.\n",
    "This step first uses the `.resample()` method to group data by quarters, starting in December, then calculates the maximum. \n",
    "In practice, the data are grouped by Dec-Jan-Feb, Mar-Apr-May, Jun-Jul-Aug, and Sep-Oct-Nov, and the maximum function is applied to each group. \n",
    "Note that the query has already restricted the STAC items to be in the Dec-Jan-Feb quarter.\n",
    "\n",
    "The second step takes the resampled data and selects all the quarters that begin in December, dropping the additional quarters that contain no data.\n",
    "\n",
    "The final step is to calculate the maximum for each pixel in each December quarter and load the data into memory using the `.compute()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample data to quarters, from beginning of December\n",
    "quarters_max = trend_data_masked.resample(time=\"QS-DEC\").max()\n",
    "\n",
    "# Only keep quaryers where the month start is DEC\n",
    "dec_quarters_max = quarters_max.sel(time=quarters_max.time.dt.month == 12)\n",
    "\n",
    "# Compute\n",
    "dec_quaters_max_evi2 = dec_quarters_max.evi2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_quaters_max_evi2.plot.imshow(col=\"time\", col_wrap=4, cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Observing the trend\n",
    "\n",
    "As in Part 2, it is useful to take the average over all pixels to see the time series. \n",
    "In this case, it is useful to use a scatter plot to see whether there is an overall trend in the index value over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_quaters_max_evi2.mean([\"x\", \"y\"]).plot.scatter(ylim=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and next steps\n",
    "\n",
    "This notebook demonstrated how to stream data direct from NASA's Earthdata STAC and perform an Earth observation analysis of land productivity, one of the key subindicators of SDG indicator 15.3.1 relating to measuring land degredation. \n",
    "\n",
    "The next steps from here would be to identify whether there is a statistically significant trend in the annual maximum EVI2 values, which could then be classified as the land becoming degraded or not degraded. This would then feed into SDG indicator 15.3.1 along with measures on land cover and carbon stocks to produce an overall assessment of the amount of degraded area as a proporition of the total area. \n",
    "\n",
    "As a user of this notebook, you may like to:\n",
    "1. Identify a new area of interest and re-run the analysis\n",
    "2. Extend the analysis to fit a trend line to the EVI2 over time\n",
    "\n",
    "You can learn more about how Earth observation can be used for SDG Indicator 15.3.1 from [Satellite Data Requirements for SDGIndicator 15.3.1](https://ceos.org/sdg/files/supportsheets/SDG_15.3), authored by the Committee on Earth Observation Satellites."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
